{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing dependency libraries\n",
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# Load the Environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# Q - Network Implementation\n",
    "\n",
    "## Creating Neural Network\n",
    "tf. reset_default_graph( )\n",
    "# tensors for inputs, weights, biases, Qtarget\n",
    "inputs = tf. placeholder( shape=[None, env. observation_space. n] , dtype=tf. float32)\n",
    "W = tf. get_variable( name=\"W\", dtype=tf. float32, shape=[env. observation_space. n, env. action_space. n] , initializer=tf. contrib. layers. xavier_initializer( ) )\n",
    "b = tf. Variable( tf. zeros( shape=[env. action_space. n] ) , dtype=tf. float32)\n",
    "qpred = tf. add( tf. matmul( inputs, W) , b)\n",
    "apred = tf. argmax( qpred, 1)\n",
    "qtar = tf. placeholder( shape=[1, env. action_space. n] , dtype=tf. float32)\n",
    "loss = tf. reduce_sum( tf. square( qtar- qpred) )\n",
    "train = tf. train. AdamOptimizer( learning_rate=0.001)\n",
    "minimizer = train. minimize( loss)\n",
    "## Training the neural network\n",
    "init = tf. global_variables_initializer( ) #initializing tensor variables\n",
    "#initializing parameters\n",
    "y = 0.5 #discount factor\n",
    "e = 0.3 #epsilon value for epsilon- greedy task\n",
    "episodes = 10000 #total number of episodes\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "with session as sess:\n",
    "    sess. run(init)\n",
    "    for i in range(episodes):\n",
    "        print('episode' + str(i))\n",
    "        s = env. reset( ) #resetting the environment at the start of each episode\n",
    "        r_total = 0 #to calculate the sum of rewards in the current episode\n",
    "        while( True) :\n",
    "            #running the Q- network created above\n",
    "            a_pred, q_pred = sess. run( [apred, qpred] , feed_dict={inputs: np. identity( env. observation_space. n) [s: s+1] })\n",
    "            #a_pred is the action prediction by the neural network\n",
    "            #q_pred contains q_values of the actions at current state ' s'\n",
    "            if np. random. uniform( low=0, high=1) < e: #performing epsilon- greedy here\n",
    "                a_pred[0] = env. action_space. sample( )\n",
    "                #exploring different action by randomly assigning them as the next action\n",
    "            s_, r, t, _ = env. step( a_pred[0] ) #action taken and new state ' s_' is encountered with a feedback reward ' r'\n",
    "            if r==0:\n",
    "                if t==True:\n",
    "                    r=- 5 #if hole make the reward more negative\n",
    "                else:\n",
    "                    r=- 1 #if block is fine/frozen then give slight negative reward to optimize the path\n",
    "            if r==1:\n",
    "                r=5 #good positive goat state reward\n",
    "            q_pred_new = sess. run( qpred, feed_dict={inputs: np. identity( env. observation_space. n) [s_: s_+1] })\n",
    "            #q_pred_new contains q_values of the actions at the new state\n",
    "            #update the Q- target value for action taken\n",
    "            targetQ = q_pred\n",
    "            max_qpredn = np. max( q_pred_new)\n",
    "            targetQ[0, a_pred[0] ] = r + y*max_qpredn\n",
    "            #this gives our targetQ\n",
    "            #train the neural network to minimize the loss\n",
    "            _ = sess. run( minimizer, feed_dict={inputs: np. identity( env. observation_space. n) [s: s+1] , qtar: targetQ})\n",
    "            s=s_\n",
    "            if t==True:\n",
    "                break\n",
    "\n",
    "#learning ends with the end of the above loop of several episodes above\n",
    "#let' s check how much our agent has learned\n",
    "print( \"Output after learning\")\n",
    "print( )\n",
    "session = tf.Session(config=config)\n",
    "session.run(init)\n",
    "s = env. reset()\n",
    "env.render()\n",
    "while( True) :\n",
    "    a = session. run( apred, feed_dict={inputs: np. identity( env. observation_space. n) [s: s+1] })\n",
    "    s_, r, t, _ = env. step(a[0])\n",
    "    print(\"===============\")\n",
    "    env.render()\n",
    "    s = s_\n",
    "    if t==True:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
